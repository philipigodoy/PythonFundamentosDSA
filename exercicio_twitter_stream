# Stream de Dados do Twitter com MongoDB, Pandas e Scikit Learn
# Preparando a Conexão com o Twitter

# Instala o pacote tweepy
pip install tweepy

# Importando os módulos Tweepy, Datetime e Json
from tweepy.streaming import StreamListener
from tweepy import OAuthHandler
from tweepy import Stream
from datatime import datetime
import json

# Adicione aqui sua Consumer Key
consumer_key = "xxxx"

# Adicione aqui sua Consumer Secret 
consumer_secret = "xxxx"

# Adicione aqui seu Access Token
access_token = "xxxx"

# Adicione aqui seu Access Token Secret
acess_token_secret = "xxxx"

# Criando as chaves de autenticação
auth = OAuthHandler(consumer_key, consumer_secret)

auth.set_access_token(access_token, acess_token_secret)

# Criando uma classe para capturar os stream de dados do Twitter e 
# armazenar no MongoDB
class MyListener(StreamListener):
	def on_data(self, dados):
		tweet = json.loads(dados)
		created_at = tweet["created_at"]
		id_str = tweet["id_str"]
		text = tweet["text"]
		obj = {"created_at" : created_at, "id_str":id_str,"text": text}
		tweetind = col.insert_one(obj).inserted_id
		print(obj)
		return True

# Criando o objeto mylistener
myListener = MyListener()

# Criando o objeto mystream
myStream = Stream(auth, listener = myListener)

## Preparando a Conexão com o MongoDB
from pymongo import MongoClient

# Criando a conexão ao MongoDB
client = MongoClient('localhost', 27017)

# Criando o banco de dados twitterdb
database = cliente.twitterdb

# Criando a collection "col"
coll = database.tweets

# Criando uma lista de palavras chave para buscar nos Tweets
keywords = ["Big Data', 'Python', 'Data Mining', 'Data Science"]

##Coletando os Tweets
# Iniciando o filtro e gravando os tweets no MongoDB
myStream.filter(track = keywords)

##--> Pressione o botão Stop
#na barra de ferramentas para encerrar a captura dos Tweets
#caso use o Jupyter Notebook, ou Ctrl + Z


#Consultando os Dados no MongoDB
myStream.disconnect()

# Verificando um documento no collection
print(coll.find_one())


#Análise de Dados com Pandas e Scikit-Learn
# criando um dataset com dados retornados do MongoDB

dataset = [{"created_at" : item["created_at"], "text":item["text"]} for item in coll.find()]

# Importando o módulo Pandas para trabalhar com datasets em Python
import pandas as pd

# Criando um dataframe a partir do dataset 
df = pd.DataFrame(dataset)
print(df)

# Importando o módulo Scikit Learn
from sklearn.feature_extraction.text import CountVectorizer

# Usando o método CountVectorizer para criar uma matriz de documentos
cv = CountVectorizer()
count_matrix = cv.fit_transform(df.text)


# Contando o número de ocorrências das principais palavras em nosso dataset
word_count = pd.DataFrame(cv.get_feature_names(), colums=["word"])
word_count["count"] = count_matrix.sum(axis = 0).tolist()[0]
word_count word_count.sort_values("count", ascending =False).reset_index(drop=True)
word_count[:50]



